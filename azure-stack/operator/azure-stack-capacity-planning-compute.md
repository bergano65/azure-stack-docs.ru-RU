---
title: Планирование вычислительной емкости в Azure Stack | Документация Майкрософт
description: Сведения о планировании емкости для развертываний Azure Stack.
services: azure-stack
documentationcenter: ''
author: prchint
manager: femila
editor: ''
ms.assetid: ''
ms.service: azure-stack
ms.workload: na
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 07/16/2019
ms.author: justinha
ms.reviewer: prchint
ms.lastreviewed: 06/13/2019
ms.openlocfilehash: dac0360bba7c24c85d1f30efbfb7fad30eb97028
ms.sourcegitcommit: cefba8d6a93efaedff303d3c605b02bd28996c5d
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/21/2019
ms.locfileid: "74299154"
---
# <a name="azure-stack-compute"></a>Службы вычислений Azure Stack

В Azure Stack поддерживаются разные [размеры виртуальных машин](https://docs.microsoft.com/azure-stack/user/azure-stack-vm-sizes). В Azure используется несколько методов ограничения ресурсов. Это предотвращает их чрезмерное использование (на уровне локальных серверов и служб). Если не ограничить потребление ресурсов клиентом, его работа может ухудшиться при чрезмерном использовании ресурсов другими клиентами. Для исходящего трафика виртуальной машины в Azure Stack предусмотрены ограничения пропускной способности, соответствующие ограничениям в Azure. Для ресурсов хранилища в Azure Stack предусмотрены ограничения числа операций ввода-вывода в секунду, которые предотвращают чрезмерное использование основных ресурсов клиентами для доступа к хранилищу.

>[!IMPORTANT]
>[Планировщик ресурсов Azure Stack](https://aka.ms/azstackcapacityplanner) не предполагает и не обеспечивает определенную производительность операций ввода-вывода.

## <a name="vm-placement"></a>Размещение виртуальной машины

Подсистема размещения Azure Stack перемещает виртуальные машины клиентов на доступные узлы.

Azure Stack учитывает два фактора при размещении виртуальных машин. Первый — это наличие у узла достаточного объема памяти для такого типа виртуальной машины. Второй — принадлежность виртуальных машин к [группе доступности](https://docs.microsoft.com/azure/virtual-machines/windows/manage-availability) или [масштабируемым наборам виртуальных машин](https://docs.microsoft.com/azure/virtual-machine-scale-sets/overview).

Чтобы добиться высокого уровня доступности рабочих систем с несколькими виртуальными машинами в Azure Stack, эти виртуальные машины размещаются в группе доступности, которая распределяет их между несколькими доменами сбоя. Домен сбоя в группе доступности определяется как отдельный узел в единице масштабирования. Azure Stack поддерживает группы доступности с максимально тремя доменами сбоя, что гарантирует согласованность с Azure. Виртуальные машины, размещенные в группе доступности, физически изолируются друг от друга и максимально равномерно распределяются между несколькими доменами сбоя (узлами Azure Stack). В случае сбоя оборудования виртуальные машины из поврежденного домена сбоя перезагружаются в других доменах сбоя, но, если это возможно, остаются в доменах сбоя, отдельных от других виртуальных машин в той же группе доступности. Когда работоспособность узла восстанавливается, виртуальные машины перераспределяются, что обеспечивает высокий уровень доступности.  

Масштабируемые наборы виртуальных машин используют группы доступности в серверной части, чтобы каждый экземпляр масштабируемого набора виртуальных машин размещался в домене сбоя. Это означает, что они используют отдельные узлы инфраструктуры Azure Stack. Например, в системе Azure Stack, состоящей из четырех узлов, создание масштабируемого набора виртуальных машин, состоящего из трех экземпляров, может завершиться сбоем из-за того, что емкости четырех узлов будет недостаточно для размещения трех экземпляров масштабируемого набора виртуальных машин на трех отдельных узлах Azure Stack. Кроме того, узлы Azure Stack можно заполнять на разных уровнях до попытки размещения. 

Azure Stack не предполагает интенсивное использование памяти. При этом допускается интенсивное использование определенного числа физических ядер. 

Так как алгоритмы подготовки не учитывают избыточную подготовку виртуальных ядер относительно физических, это отношение в каждом узле может быть разным. Корпорация Майкрософт не предоставляет рекомендации по соотношению физических и виртуальных ядер из-за отличающихся рабочих нагрузок и требований к уровню обслуживания. 

## <a name="consideration-for-total-number-of-vms"></a>Рекомендации по общему числу виртуальных машин 

Существует новый подход для точного планирования емкости Azure Stack. Ограничение на количество создаваемых виртуальных машин было введено в обновлении 1901 и будет действовать во всех последующих обновлениях. Данное ограничение является временным и предназначено для предотвращения сбоев в решении. Источник ошибок стабильности при большом количестве виртуальных машин обрабатывается, но точное время его устранения еще не определено. Теперь действует ограничение в 60 виртуальных машин на один сервер, а максимальное общее число виртуальных машин равно 700. Например, ограничение 8 сервера виртуальной машины Azure Stack составит 480 (8 * 60). А для решения из 12 и 16 серверов Azure Stack ограничение составит 700. Ограничение было создано, чтобы учесть все возможные вычислительные мощности, например, резерв для устойчивости и соотношение виртуальных и физических ресурсов ЦП, которое оператор хотел бы сохранить в метке. Дополнительные сведения см. в разделе о новом выпуске планировщика ресурсов. 

Если предел масштабирования виртуальной машины постигнут, в качестве результата будут возвращены следующие коды ошибок: VMsPerScaleUnitLimitExceeded, VMsPerScaleUnitNodeLimitExceeded.

## <a name="considerations-for-deallocation"></a>Рекомендации по освобождению

Если виртуальная машина находится в _освобожденном_ состоянии, ресурсы памяти не используются. Это позволяет разместить в системе другие виртуальные машины. 

Если освобожденная виртуальная машина снова запускается, то использует память или выделенные ресурсы как новая виртуальная машина в системе и потребляет доступную память. 

Если доступной памяти нет, виртуальная машина не запустится.

## <a name="azure-stack-memory"></a>Память Azure Stack 

В Azure Stack работают виртуальные машины, которые были успешно подготовлены. Например, если узел находится в автономном режиме из-за сбоя оборудования, Azure Stack попытается перезапустить эту виртуальную машину на другом узле. Второй пример — установка исправлений и обновлений программного обеспечения Azure Stack. Если нужно перезагрузить физический узел, будет предпринята попытка переместить виртуальные машины, работающие на этом узле, на другой узел, доступный в решении.   

Такое управление виртуальными машинами или их перемещение возможно только при наличии зарезервированного объема памяти для перезапуска или миграции. Часть всей памяти узла резервируется и становится недоступной для размещения виртуальной машины клиента. 

Вы можете просмотреть круговую диаграмму на портале администрирования, чтобы оценить неиспользуемую и используемую в Azure Stack память. На схеме ниже показан объем физической памяти в единице масштабирования Azure Stack в Azure Stack:

![Объем физической памяти](media/azure-stack-capacity-planning/physical-memory-capacity.png)

Используемая память состоит из нескольких компонентов. Следующие компоненты используют память в соответствующем разделе круговой диаграммы:  

 -  Резервирование или использование ОС узла — это объем памяти, используемый операционной системой (ОС) узла, таблицами страниц виртуальной памяти, процессами в ОС узла, а также кэшем памяти локальных дисковых пространств. Так как это значение зависит от памяти, используемой другими работающими на узле процессами Hyper-V, оно может меняться.
 - Службы инфраструктуры — это инфраструктура виртуальных машин, работающих в Azure Stack. В выпуске Azure Stack 1904 для нее предусмотрено порядка 31 виртуальной машины. Вместе они потребляют 242 ГБ + (4 ГБ x число узлов) памяти. Объем памяти, используемый компонентом служб инфраструктуры, может измениться, так как мы работаем над улучшением масштабируемости и устойчивости служб инфраструктуры.
 - Резерв для устойчивости — Azure Stack резервирует часть памяти для обеспечения доступности клиента во время сбоя одного узла, а также во время установки исправлений и обновлений для обеспечения успешной динамической миграции виртуальных машин.
 - Виртуальные машины клиента — это клиентские виртуальные машины, созданные пользователями Azure Stack. Наряду с работающими виртуальными машинами память также потребляют виртуальные машины, которые работают в структуре. Это означает, что виртуальные машины, которые находятся в состоянии "Создание" или "Сбой" либо работа которых завершена из гостевой ОС, также будут потреблять память. При этом виртуальные машины, распределение которых было отменено путем остановки на портале или с помощью Powershell или CLI, не потребляют память Azure Stack.
 - Точки восстановления надстроек — это виртуальные машины, развернутые для точек восстановления надстроек, например SQL, MySQL, Службы приложений и т. д.


Лучший способ оценить потребление памяти на портале — воспользоваться [планировщиком ресурсов Azure Stack](https://aka.ms/azstackcapacityplanner), чтобы просмотреть влияние разных рабочих нагрузок. Следующие вычисления соответствуют расчетам планировщика.

Это вычисление позволяет рассчитать общий объем доступной памяти для размещения виртуальных машин клиента. Этот объем памяти предназначен для всей единицы масштабирования Azure Stack. 


  Доступный объем памяти для размещения виртуальных машин = общий объем памяти на узле - резерв для устойчивости - объем памяти, используемый работающими виртуальными машинами клиента - служебные данные инфраструктуры Azure Stack <sup>1</sup>

  Резерв устойчивости = H + R * ((N-1) * H) + V * (N-2)

> Описание
> - H = размер памяти на одном сервере
> - N = размер единицы масштабирования (число серверов)
> - R = резерв для нагрузки ОС, который составляет 0,15 в этой формуле <sup>2</sup>
> - V = самая большая виртуальная машина в единице масштабирования

  <sup>1</sup> Служебные данные инфраструктуры Azure Stack: 242 ГБ + (4 ГБ x число узлов). Для размещения инфраструктуры Azure Stack используется приблизительно 31 виртуальная машина. Все они в совокупности потребляют примерно 242 ГБ памяти (4 ГБ х число узлов) и имеют 146 виртуальных ядер. Такое количество виртуальных машин обосновывается необходимостью в разделении служб с целью удовлетворить требования к безопасности, масштабируемости, обслуживанию и применению исправлений. Внутренняя структура служб позволяет вводить в будущем новые службы инфраструктуры по мере их разработки. 

  <sup>2</sup> Резерв для нагрузки ОС составляет 15 % (0,15) памяти узла. Значение резерва операционной системы является приблизительным и зависит от объема физической памяти на сервере и общих накладных расходов операционной системы.


Значение V (самая большая виртуальная машина в единице масштабирования) меняется динамически в зависимости от максимального размера памяти виртуальной машины в клиенте. Например, оно может быть равно 7 ГБ, 112 ГБ или соответствовать другому размеру памяти виртуальной машины, поддерживаемому решением Azure Stack. Изменение самой большой виртуальной машины в структуре Azure Stack приведет к увеличению резерва для устойчивости, а также увеличению объема памяти самой виртуальной машины. 

## <a name="frequently-asked-questions"></a>Часто задаваемые вопросы

**Вопрос**. Мой клиент развернул новую виртуальную машину. Сколько времени потребуется на то, чтобы на диаграмме возможностей на портале администрирования отобразилась оставшаяся емкость?

**Ответ**. Колонка емкости обновляется каждые 15 минут, поэтому учитывайте это при оценке доступной емкости.

**Вопрос**. Число развернутых виртуальных машин в Azure Stack осталось прежним, но объем доступной емкости меняется. Почему?

**Ответ**. Объем доступной памяти для размещения виртуальных машин зависит от множества факторов, одним из которых является резерв ОС узла. Это значение зависит от памяти, используемой другими работающими на узле процессами Hyper-V, объем которой может меняться.

**Вопрос**. Какое состояние должны иметь виртуальные машины клиента для потребления памяти?

**Ответ**. Наряду с работающими виртуальными машинами память также потребляют виртуальные машины, которые работают в структуре. Это значит, что виртуальные машины, которые находятся в состоянии "Создание" или "Сбой", либо работа которых завершена из гостевой ОС (а не с отменой распределения путем остановки на портале или с помощью Powershell или CLI), будут потреблять память.

**Вопрос**. У меня есть среда Azure Stack с 4 узлами. Мой клиент имеет три виртуальные машины, которые используют 56 ГБ ОЗУ (D5_v2) каждая. Размер одной из виртуальных машин был увеличен до 112 ГБ ОЗУ (D14_v2), а в колонке емкости на панели мониторинга значение доступной памяти уменьшилось на 168 ГБ. Последующее изменение размера двух других виртуальных машин D5_v2 до размера D14_v2 привело к увеличению потребления памяти всего на 56 ГБ ОЗУ для каждой из виртуальных машин. Чем это обусловлено?

**Ответ**. Объем доступной памяти определяется резервом для устойчивости Azure Stack. Резерв для устойчивости в свою очередь определяется размером крупнейшей виртуальной машины в метке Azure Stack. Изначально самая крупная виртуальная машина в метке занимала 56 ГБ. После изменения размера виртуальной машины до 112 ГБ увеличился не только объем памяти, используемый такой виртуальной машиной клиента, но и объем резерва для устойчивости. В итоге значение увеличилось на 56 ГБ (разница при изменении размера виртуальной машины клиента с 56 ГБ до 112 ГБ) + 112 ГБ резерва для устойчивости. При последующем изменении размера виртуальных машин объем памяти, используемой крупнейшей виртуальной машиной, не менялся, поэтому общий резерв для устойчивости также не увеличивался. Объем потребляемой памяти изменился только с учетом увеличения размера виртуальной машины клиента (56 ГБ). 


> [!NOTE]
> Требования к планированию ресурсов для сетевого взаимодействия минимальны, так как настраивается только размер общедоступного виртуального IP-адреса. См. подробнее о [добавлении дополнительных общедоступных IP-адресов уровня экземпляра в Azure Stack](azure-stack-add-ips.md).

## <a name="next-steps"></a>Дополнительная информация
См. подробнее о [хранилище Azure Stack](azure-stack-capacity-planning-storage.md).
